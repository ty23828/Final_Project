import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from matplotlib import colors
from scipy.spatial import cKDTree
import warnings
warnings.filterwarnings('ignore')

def _setup_map(ax, lon_min, lon_max, lat_min, lat_max):
    """Setup map with Portugal boundaries and features"""
    ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())
    
    # Add map features
    ax.add_feature(cfeature.COASTLINE, linewidth=0.8, color='black')
    ax.add_feature(cfeature.BORDERS, linewidth=0.6, color='gray')
    ax.add_feature(cfeature.OCEAN, color='lightblue', alpha=0.3)
    ax.add_feature(cfeature.LAND, color='lightgray', alpha=0.1)
    
    # Add gridlines
    gl = ax.gridlines(draw_labels=True, linewidth=0.5, alpha=0.7)
    gl.top_labels = False
    gl.right_labels = False

def filter_portugal_mainland(data):
    """Filter data to include only Portugal mainland, excluding islands"""
    
    print("   Filtering for Portugal mainland only...")
    
    # Portugal mainland boundaries (excluding Azores and Madeira)
    mainland_bounds = {
        'lat_min': 36.5,   # Southern mainland boundary
        'lat_max': 42.2,   # Northern mainland boundary
        'lon_min': -9.6,   # Western mainland boundary
        'lon_max': -6.0    # Eastern mainland boundary
    }
    
    original_count = len(data)
    
    # Apply mainland filter
    mainland_data = data[
        (data['latitude'] >= mainland_bounds['lat_min']) &
        (data['latitude'] <= mainland_bounds['lat_max']) &
        (data['longitude'] >= mainland_bounds['lon_min']) &
        (data['longitude'] <= mainland_bounds['lon_max'])
    ].copy()
    
    filtered_count = len(mainland_data)
    removed_count = original_count - filtered_count
    
    print(f"   Mainland filter applied:")
    print(f"      Original points: {original_count:,}")
    print(f"      Mainland points: {filtered_count:,}")
    print(f"      Removed (islands): {removed_count:,} ({removed_count/original_count*100:.1f}%)")
    print(f"      Mainland bounds: lat [{mainland_bounds['lat_min']}, {mainland_bounds['lat_max']}], "
          f"lon [{mainland_bounds['lon_min']}, {mainland_bounds['lon_max']}]")
    
    return mainland_data

def load_data():
    """Load FWI and LitPop exposure data"""
    
    print("Loading vulnerability analysis data...")
    
    try:
        # Load 1km FWI data
        print("   Loading FWI data...")
        fwi_data = pd.read_csv("fwi_1km_predictions_random_forest.csv")
        
        # Handle time column
        if 'date' in fwi_data.columns:
            fwi_data['time'] = pd.to_datetime(fwi_data['date'])
        elif 'time' in fwi_data.columns:
            fwi_data['time'] = pd.to_datetime(fwi_data['time'])
        
        print(f"   FWI data loaded: {fwi_data.shape[0]:,} records")
        print(f"      Date range: {fwi_data['time'].min()} to {fwi_data['time'].max()}")
        print(f"      Spatial extent: lat [{fwi_data['latitude'].min():.3f}, {fwi_data['latitude'].max():.3f}], "
              f"lon [{fwi_data['longitude'].min():.3f}, {fwi_data['longitude'].max():.3f}]")
        
        # Filter FWI data for mainland Portugal only
        fwi_data = filter_portugal_mainland(fwi_data)
        
        # Load LitPop exposure data
        print("   Loading LitPop exposure data...")
        exposure_data = pd.read_csv("LitPop_pc_30arcsec_PRT.csv")
        
        print(f"   Exposure data loaded: {exposure_data.shape[0]:,} records")
        print(f"      Columns: {list(exposure_data.columns)}")
        print(f"      Spatial extent: lat [{exposure_data['latitude'].min():.3f}, {exposure_data['latitude'].max():.3f}], "
              f"lon [{exposure_data['longitude'].min():.3f}, {exposure_data['longitude'].max():.3f}]")
        
        # Filter exposure data for mainland Portugal only
        exposure_data = filter_portugal_mainland(exposure_data)
        
        return fwi_data, exposure_data
        
    except Exception as e:
        print(f"Error loading data: {e}")
        return None, None

def spatial_matching(fwi_data, exposure_data, target_date=None):
    """Match LitPop exposure points to nearest FWI values using KDTree"""
    
    print("Performing spatial matching...")
    
    # Filter FWI data for specific date if provided
    if target_date:
        target_date = pd.to_datetime(target_date)
        fwi_filtered = fwi_data[fwi_data['time'].dt.date == target_date.date()].copy()
        print(f"   Using FWI data for date: {target_date.date()}")
    else:
        # Use mean FWI for 2017 if no specific date
        print("   Calculating mean FWI for 2017...")
        fwi_col = 'fwi_predicted' if 'fwi_predicted' in fwi_data.columns else 'fwi'
        fwi_filtered = fwi_data.groupby(['latitude', 'longitude'])[fwi_col].mean().reset_index()
        fwi_filtered.rename(columns={fwi_col: 'fwi_mean'}, inplace=True)
    
    print(f"   FWI points for matching: {len(fwi_filtered):,}")
    print(f"   Exposure points to match: {len(exposure_data):,}")
    
    # Create coordinate arrays for KDTree
    fwi_coords = np.column_stack((fwi_filtered['latitude'].values, fwi_filtered['longitude'].values))
    exposure_coords = np.column_stack((exposure_data['latitude'].values, exposure_data['longitude'].values))
    
    # Build KDTree for efficient nearest neighbor search
    print("   Building KDTree for spatial matching...")
    tree = cKDTree(fwi_coords)
    
    # Find nearest FWI point for each exposure point
    print("   Finding nearest neighbors...")
    distances, indices = tree.query(exposure_coords, k=1)
    
    # Prepare matched data
    matched_data = exposure_data.copy()
    
    # Add FWI values and distances
    if target_date:
        fwi_col = 'fwi_predicted' if 'fwi_predicted' in fwi_filtered.columns else 'fwi'
        matched_data['fwi_value'] = fwi_filtered.iloc[indices][fwi_col].values
        matched_data['match_date'] = target_date.date()
    else:
        matched_data['fwi_value'] = fwi_filtered.iloc[indices]['fwi_mean'].values
        matched_data['match_date'] = '2017_mean'
    
    matched_data['match_distance_km'] = distances * 111.32  # Convert degrees to km (approximate)
    matched_data['nearest_fwi_lat'] = fwi_filtered.iloc[indices]['latitude'].values
    matched_data['nearest_fwi_lon'] = fwi_filtered.iloc[indices]['longitude'].values
    
    # Get exposure column name (might be different in different datasets)
    exposure_cols = [col for col in exposure_data.columns if 'value' in col.lower() or 'exposure' in col.lower()]
    if not exposure_cols:
        # Try other common names
        exposure_cols = [col for col in exposure_data.columns if any(x in col.lower() for x in ['pop', 'asset', 'gdp', 'income'])]
    
    if exposure_cols:
        exposure_col = exposure_cols[0]
        matched_data['exposure_value'] = matched_data[exposure_col]
        print(f"   Using exposure column: '{exposure_col}'")
    else:
        print("   No obvious exposure column found, using placeholder")
        matched_data['exposure_value'] = 1.0
        exposure_col = 'placeholder'
    
    print(f"   Spatial matching completed!")
    print(f"      Mean matching distance: {matched_data['match_distance_km'].mean():.2f} km")
    print(f"      Max matching distance: {matched_data['match_distance_km'].max():.2f} km")
    print(f"      FWI range: [{matched_data['fwi_value'].min():.2f}, {matched_data['fwi_value'].max():.2f}]")
    print(f"      Exposure range: [{matched_data['exposure_value'].min():.2e}, {matched_data['exposure_value'].max():.2e}]")
    
    return matched_data, exposure_col

def calculate_vulnerability(matched_data, vulnerability_method='simple'):
    """Calculate vulnerability indicators"""
    
    print("Calculating vulnerability indicators...")
    
    vulnerability_data = matched_data.copy()
    
    if vulnerability_method == 'simple':
        # Simple method: Risk = FWI × Exposure
        vulnerability_data['risk_simple'] = vulnerability_data['fwi_value'] * vulnerability_data['exposure_value']
        
        print("   Simple vulnerability calculation: Risk = FWI × Exposure")
        
    elif vulnerability_method == 'normalized':
        # Normalized method: Scale both factors to 0-1 range first
        fwi_normalized = (vulnerability_data['fwi_value'] - vulnerability_data['fwi_value'].min()) / \
                        (vulnerability_data['fwi_value'].max() - vulnerability_data['fwi_value'].min())
        
        exposure_normalized = (vulnerability_data['exposure_value'] - vulnerability_data['exposure_value'].min()) / \
                             (vulnerability_data['exposure_value'].max() - vulnerability_data['exposure_value'].min())
        
        vulnerability_data['fwi_normalized'] = fwi_normalized
        vulnerability_data['exposure_normalized'] = exposure_normalized
        vulnerability_data['risk_normalized'] = fwi_normalized * exposure_normalized
        
        print("   Normalized vulnerability calculation: Risk = FWI_norm × Exposure_norm")
        
    elif vulnerability_method == 'weighted':
        # Weighted method: Give different weights to FWI and exposure
        fwi_weight = 0.7  # Higher weight for fire weather
        exposure_weight = 0.3
        
        fwi_normalized = (vulnerability_data['fwi_value'] - vulnerability_data['fwi_value'].min()) / \
                        (vulnerability_data['fwi_value'].max() - vulnerability_data['fwi_value'].min())
        
        exposure_normalized = (vulnerability_data['exposure_value'] - vulnerability_data['exposure_value'].min()) / \
                             (vulnerability_data['exposure_value'].max() - vulnerability_data['exposure_value'].min())
        
        vulnerability_data['risk_weighted'] = (fwi_weight * fwi_normalized) * (exposure_weight * exposure_normalized)
        
        print(f"   Weighted vulnerability: Risk = {fwi_weight}×FWI + {exposure_weight}×Exposure")
    
    # Calculate percentiles for risk classification
    if 'risk_simple' in vulnerability_data.columns:
        risk_col = 'risk_simple'
    elif 'risk_normalized' in vulnerability_data.columns:
        risk_col = 'risk_normalized'
    else:
        risk_col = 'risk_weighted'
    
    # Define risk categories based on percentiles
    vulnerability_data['risk_percentile'] = vulnerability_data[risk_col].rank(pct=True) * 100
    
    # Risk categories
    conditions = [
        vulnerability_data['risk_percentile'] >= 90,
        vulnerability_data['risk_percentile'] >= 75,
        vulnerability_data['risk_percentile'] >= 50,
        vulnerability_data['risk_percentile'] >= 25,
        vulnerability_data['risk_percentile'] < 25
    ]
    
    choices = ['Very High', 'High', 'Moderate', 'Low', 'Very Low']
    vulnerability_data['risk_category'] = np.select(conditions, choices, default='Low')
    
    print(f"   Vulnerability calculation completed!")
    print(f"      Risk categories distribution:")
    category_counts = vulnerability_data['risk_category'].value_counts()
    for category, count in category_counts.items():
        print(f"        {category}: {count:,} points ({count/len(vulnerability_data)*100:.1f}%)")
    
    return vulnerability_data

def create_vulnerability_maps(vulnerability_data, exposure_col, save_data=True):
    """Create comprehensive vulnerability visualization maps"""
    
    print("Creating vulnerability maps...")
    
    # Portugal mainland extent (no islands)
    mainland_bounds = {
        'lat_min': 36.5, 'lat_max': 42.2,
        'lon_min': -9.6, 'lon_max': -6.0
    }
    
    # Use mainland bounds for map extent with small buffer
    lon_min, lon_max = mainland_bounds['lon_min'] - 0.1, mainland_bounds['lon_max'] + 0.1
    lat_min, lat_max = mainland_bounds['lat_min'] - 0.1, mainland_bounds['lat_max'] + 0.1
    
    print(f"   Map extent (mainland): lat [{lat_min:.1f}, {lat_max:.1f}], lon [{lon_min:.1f}, {lon_max:.1f}]")
    
    # Create figure with 2x2 subplots
    fig = plt.figure(figsize=(20, 16))
    
    # Remove extreme outliers and handle zero values for better visualization
    data_clean = vulnerability_data.copy()
    
    # Additional mainland filter on visualization data (double-check)
    data_clean = data_clean[
        (data_clean['latitude'] >= mainland_bounds['lat_min']) &
        (data_clean['latitude'] <= mainland_bounds['lat_max']) &
        (data_clean['longitude'] >= mainland_bounds['lon_min']) &
        (data_clean['longitude'] <= mainland_bounds['lon_max'])
    ].copy()
    
    # Handle zero and negative exposure values
    print(f"   Original exposure value range: [{data_clean['exposure_value'].min():.2e}, {data_clean['exposure_value'].max():.2e}]")
    
    # Remove points with zero or negative exposure values
    zero_exposure_count = (data_clean['exposure_value'] <= 0).sum()
    if zero_exposure_count > 0:
        print(f"   Removing {zero_exposure_count:,} points with zero/negative exposure values")
        data_clean = data_clean[data_clean['exposure_value'] > 0].copy()
    
    if len(data_clean) == 0:
        print("   No valid data points after removing zero exposure values")
        return vulnerability_data
    
    # Cap extreme exposure values at 99th percentile
    exposure_99 = data_clean['exposure_value'].quantile(0.99)
    data_clean['exposure_value_capped'] = np.minimum(data_clean['exposure_value'], exposure_99)
    
    print(f"   Visualization data (mainland only): {len(data_clean):,} points")
    print(f"      FWI range: [{data_clean['fwi_value'].min():.2f}, {data_clean['fwi_value'].max():.2f}]")
    print(f"      Exposure range (capped): [{data_clean['exposure_value_capped'].min():.2e}, {data_clean['exposure_value_capped'].max():.2e}]")
    
    # 1. FWI Distribution Map - Top Left
    ax1 = fig.add_subplot(2, 2, 1, projection=ccrs.PlateCarree())
    
    # FWI colormap
    fwi_colors = ['#2E8B57', '#32CD32', '#FFD700', '#FF8C00', '#FF4500', '#DC143C', '#8B0000']
    fwi_levels = [0, 5, 12, 24, 38, 50, 75, 100]
    fwi_cmap = colors.ListedColormap(fwi_colors)
    fwi_norm = colors.BoundaryNorm(fwi_levels, fwi_cmap.N)
    
    scatter1 = ax1.scatter(data_clean['longitude'], data_clean['latitude'],
                          c=data_clean['fwi_value'], cmap=fwi_cmap, norm=fwi_norm,
                          s=12, alpha=0.8, transform=ccrs.PlateCarree())  # Increased point size for mainland
    
    _setup_map(ax1, lon_min, lon_max, lat_min, lat_max)
    ax1.set_title('Fire Weather Index (FWI)\n1km Resolution - Portugal Mainland', 
                 fontsize=14, fontweight='bold')
    
    # Add FWI colorbar
    cbar1 = fig.colorbar(scatter1, ax=ax1, shrink=0.8, pad=0.02)
    cbar1.set_label('FWI Value', fontsize=12)
    
    # Add FWI statistics
    fwi_stats = f"Mean: {data_clean['fwi_value'].mean():.1f}\nMedian: {data_clean['fwi_value'].median():.1f}\nMax: {data_clean['fwi_value'].max():.1f}"
    ax1.text(0.02, 0.98, fwi_stats, transform=ax1.transAxes,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
            fontsize=10, verticalalignment='top')
    
    # 2. Economic Exposure Map - Top Right
    ax2 = fig.add_subplot(2, 2, 2, projection=ccrs.PlateCarree())
    
    # Exposure colormap (log scale for better visualization)
    exposure_cmap = plt.cm.YlOrRd
    
    # Safe division check for log scale decision
    exposure_min = data_clean['exposure_value_capped'].min()
    exposure_max = data_clean['exposure_value_capped'].max()
    
    # Use log scale if exposure values span many orders of magnitude and min > 0
    if exposure_min > 0 and (exposure_max / exposure_min) > 1000:
        # Use log scale
        exposure_log = np.log10(data_clean['exposure_value_capped'])
        scatter2 = ax2.scatter(data_clean['longitude'], data_clean['latitude'],
                              c=exposure_log, cmap=exposure_cmap,
                              s=12, alpha=0.8, transform=ccrs.PlateCarree())
        cbar2_label = f'Log10({exposure_col})'
    else:
        # Use linear scale
        scatter2 = ax2.scatter(data_clean['longitude'], data_clean['latitude'],
                              c=data_clean['exposure_value_capped'], cmap=exposure_cmap,
                              s=12, alpha=0.8, transform=ccrs.PlateCarree())
        cbar2_label = f'{exposure_col}'
    
    _setup_map(ax2, lon_min, lon_max, lat_min, lat_max)
    ax2.set_title('Economic Exposure\nLitPop Dataset - Portugal Mainland', 
                 fontsize=14, fontweight='bold')
    
    # Add exposure colorbar
    cbar2 = fig.colorbar(scatter2, ax=ax2, shrink=0.8, pad=0.02)
    cbar2.set_label(cbar2_label, fontsize=12)
    
    # Add exposure statistics
    exposure_stats = f"Mean: {data_clean['exposure_value'].mean():.2e}\nMedian: {data_clean['exposure_value'].median():.2e}\nMax: {data_clean['exposure_value'].max():.2e}"
    ax2.text(0.02, 0.98, exposure_stats, transform=ax2.transAxes,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
            fontsize=10, verticalalignment='top')
    
    # 3. Vulnerability Risk Map - Bottom Left
    ax3 = fig.add_subplot(2, 2, 3, projection=ccrs.PlateCarree())
    
    # Determine risk column
    risk_col = None
    for col in ['risk_simple', 'risk_normalized', 'risk_weighted']:
        if col in data_clean.columns:
            risk_col = col
            break
    
    if risk_col:
        # Risk colormap
        risk_cmap = plt.cm.Reds
        
        # Safe division check for risk values
        risk_min = data_clean[risk_col].min()
        risk_max = data_clean[risk_col].max()
        
        # Use log scale for risk if needed and min > 0
        if risk_min > 0 and (risk_max / risk_min) > 1000:
            risk_log = np.log10(data_clean[risk_col])
            scatter3 = ax3.scatter(data_clean['longitude'], data_clean['latitude'],
                                  c=risk_log, cmap=risk_cmap,
                                  s=12, alpha=0.8, transform=ccrs.PlateCarree())
            cbar3_label = f'Log10({risk_col.replace("_", " ").title()})'
        else:
            scatter3 = ax3.scatter(data_clean['longitude'], data_clean['latitude'],
                                  c=data_clean[risk_col], cmap=risk_cmap,
                                  s=12, alpha=0.8, transform=ccrs.PlateCarree())
            cbar3_label = risk_col.replace("_", " ").title()
        
        _setup_map(ax3, lon_min, lon_max, lat_min, lat_max)
        ax3.set_title('Vulnerability Risk\nFWI × Economic Exposure - Mainland', 
                     fontsize=14, fontweight='bold')
        
        # Add risk colorbar
        cbar3 = fig.colorbar(scatter3, ax=ax3, shrink=0.8, pad=0.02)
        cbar3.set_label(cbar3_label, fontsize=12)
        
        # Add risk statistics
        risk_stats = f"Mean: {data_clean[risk_col].mean():.2e}\nMedian: {data_clean[risk_col].median():.2e}\nMax: {data_clean[risk_col].max():.2e}"
        ax3.text(0.02, 0.98, risk_stats, transform=ax3.transAxes,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
                fontsize=10, verticalalignment='top')
    
    # 4. Risk Categories Map - Bottom Right
    ax4 = fig.add_subplot(2, 2, 4, projection=ccrs.PlateCarree())
    
    # Risk category colormap
    category_colors = {'Very Low': '#2166ac', 'Low': '#67a9cf', 'Moderate': '#f7f7f7', 
                      'High': '#fdbf6f', 'Very High': '#d73027'}
    
    unique_categories = data_clean['risk_category'].unique()
    category_list = ['Very Low', 'Low', 'Moderate', 'High', 'Very High']
    colors_list = [category_colors[cat] for cat in category_list if cat in unique_categories]
    
    # Handle case where we might have no categories
    if len(colors_list) == 0:
        colors_list = ['#f7f7f7']  # Default color
        unique_categories = ['Unknown']
    
    category_cmap = colors.ListedColormap(colors_list)
    category_norm = colors.BoundaryNorm(range(len(unique_categories) + 1), category_cmap.N)
    
    # Map categories to numbers for plotting
    category_mapping = {cat: i for i, cat in enumerate(sorted(unique_categories))}
    category_numbers = data_clean['risk_category'].map(category_mapping)
    
    scatter4 = ax4.scatter(data_clean['longitude'], data_clean['latitude'],
                          c=category_numbers, cmap=category_cmap, norm=category_norm,
                          s=12, alpha=0.8, transform=ccrs.PlateCarree())
    
    _setup_map(ax4, lon_min, lon_max, lat_min, lat_max)
    ax4.set_title('Risk Categories\nPercentile-based Classification - Mainland', 
                 fontsize=14, fontweight='bold')
    
    # Add category colorbar
    cbar4 = fig.colorbar(scatter4, ax=ax4, shrink=0.8, pad=0.02)
    cbar4.set_ticks(range(len(unique_categories)))
    cbar4.set_ticklabels(sorted(unique_categories))
    cbar4.set_label('Risk Category', fontsize=12)
    
    # Add category statistics
    if len(data_clean['risk_category'].value_counts()) > 0:
        top_category = data_clean['risk_category'].value_counts().index[0]
        top_count = data_clean['risk_category'].value_counts().iloc[0]
        category_stats = f"Most common: {top_category}\n({top_count:,} points)\nTotal: {len(data_clean):,} points"
    else:
        category_stats = f"Total: {len(data_clean):,} points"
    
    ax4.text(0.02, 0.98, category_stats, transform=ax4.transAxes,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.9),
            fontsize=10, verticalalignment='top')
    
    # Add overall title
    plt.suptitle('Fire Weather Vulnerability Analysis - Portugal Mainland\nFWI × Economic Exposure Assessment (Excluding Islands)', 
                fontsize=16, fontweight='bold', y=0.98)
    
    # Adjust layout
    plt.tight_layout()
    
    # Save the plot
    filename = "fwi_vulnerability_analysis_mainland.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"   Vulnerability maps saved: {filename}")
    
    # Save processed data
    if save_data:
        output_filename = "vulnerability_analysis_results_mainland.csv"
        vulnerability_data.to_csv(output_filename, index=False)
        print(f"   Vulnerability data saved: {output_filename}")
    
    return vulnerability_data

def create_vulnerability_summary(vulnerability_data):
    """Create summary statistics and insights"""
    
    print("Creating vulnerability analysis summary...")
    
    # Find risk column
    risk_col = None
    for col in ['risk_simple', 'risk_normalized', 'risk_weighted']:
        if col in vulnerability_data.columns:
            risk_col = col
            break
    
    if not risk_col:
        print("   No risk column found for summary")
        return
    
    # High-risk areas analysis
    high_risk_threshold = vulnerability_data[risk_col].quantile(0.9)
    high_risk_areas = vulnerability_data[vulnerability_data[risk_col] >= high_risk_threshold]
    
    print(f"\nHIGH-RISK AREAS ANALYSIS - PORTUGAL MAINLAND (Top 10%):")
    print(f"   • Number of high-risk points: {len(high_risk_areas):,}")
    print(f"   • Risk threshold: {high_risk_threshold:.2e}")
    print(f"   • Mean FWI in high-risk areas: {high_risk_areas['fwi_value'].mean():.2f}")
    print(f"   • Mean exposure in high-risk areas: {high_risk_areas['exposure_value'].mean():.2e}")
    
    # Geographic distribution of high-risk areas
    print(f"   • Latitude range: [{high_risk_areas['latitude'].min():.3f}, {high_risk_areas['latitude'].max():.3f}]")
    print(f"   • Longitude range: [{high_risk_areas['longitude'].min():.3f}, {high_risk_areas['longitude'].max():.3f}]")
    
    # Identify regions (rough geographical classification)
    print(f"\nHIGH-RISK REGIONAL DISTRIBUTION:")
    
    # Northern Portugal: lat > 41.0
    north_high_risk = high_risk_areas[high_risk_areas['latitude'] > 41.0]
    # Central Portugal: 39.5 < lat <= 41.0
    central_high_risk = high_risk_areas[(high_risk_areas['latitude'] > 39.5) & (high_risk_areas['latitude'] <= 41.0)]
    # Southern Portugal: lat <= 39.5
    south_high_risk = high_risk_areas[high_risk_areas['latitude'] <= 39.5]
    
    print(f"   • Northern Portugal (lat > 41.0°): {len(north_high_risk):,} points ({len(north_high_risk)/len(high_risk_areas)*100:.1f}%)")
    print(f"   • Central Portugal (39.5° < lat ≤ 41.0°): {len(central_high_risk):,} points ({len(central_high_risk)/len(high_risk_areas)*100:.1f}%)")
    print(f"   • Southern Portugal (lat ≤ 39.5°): {len(south_high_risk):,} points ({len(south_high_risk)/len(high_risk_areas)*100:.1f}%)")
    
    # Risk category summary
    print(f"\nRISK CATEGORY DISTRIBUTION (MAINLAND):")
    category_summary = vulnerability_data['risk_category'].value_counts()
    for category, count in category_summary.items():
        percentage = count / len(vulnerability_data) * 100
        print(f"   • {category}: {count:,} points ({percentage:.1f}%)")
    
    # Correlation analysis
    corr_fwi_exposure = vulnerability_data['fwi_value'].corr(vulnerability_data['exposure_value'])
    corr_fwi_risk = vulnerability_data['fwi_value'].corr(vulnerability_data[risk_col])
    corr_exposure_risk = vulnerability_data['exposure_value'].corr(vulnerability_data[risk_col])
    
    print(f"\nCORRELATION ANALYSIS:")
    print(f"   • FWI vs Exposure: {corr_fwi_exposure:.3f}")
    print(f"   • FWI vs Risk: {corr_fwi_risk:.3f}")
    print(f"   • Exposure vs Risk: {corr_exposure_risk:.3f}")
    
    # Spatial clustering analysis (simple)
    high_risk_lat_center = high_risk_areas['latitude'].mean()
    high_risk_lon_center = high_risk_areas['longitude'].mean()
    
    print(f"\nHIGH-RISK HOTSPOTS (MAINLAND):")
    print(f"   • Center of high-risk areas: {high_risk_lat_center:.3f}°N, {abs(high_risk_lon_center):.3f}°W")
    
    # Find top 10 highest risk points
    top_risk_points = vulnerability_data.nlargest(10, risk_col)
    print(f"\nTOP 10 HIGHEST RISK LOCATIONS (MAINLAND):")
    for i, (_, point) in enumerate(top_risk_points.iterrows(), 1):
        print(f"   {i:2d}. Lat: {point['latitude']:.3f}, Lon: {point['longitude']:.3f}, "
              f"FWI: {point['fwi_value']:.1f}, Risk: {point[risk_col]:.2e}")
    
    print(f"\nMAINLAND COVERAGE SUMMARY:")
    print(f"   • Total mainland points analyzed: {len(vulnerability_data):,}")
    print(f"   • Latitude range: [{vulnerability_data['latitude'].min():.2f}°, {vulnerability_data['latitude'].max():.2f}°]")
    print(f"   • Longitude range: [{vulnerability_data['longitude'].min():.2f}°, {vulnerability_data['longitude'].max():.2f}°]")
    
def main():
    """Main function for vulnerability analysis"""
    
    print("Fire Weather Vulnerability Analysis - Portugal Mainland")
    print("=" * 60)
    
    try:
        # 1. Load data
        fwi_data, exposure_data = load_data()
        
        if fwi_data is None or exposure_data is None:
            print("Failed to load data. Exiting.")
            return
        
        # 2. Spatial matching
        # Option 1: Use a specific date
        # matched_data, exposure_col = spatial_matching(fwi_data, exposure_data, target_date='2017-07-21')
        
        # Option 2: Use annual mean (recommended for general analysis)
        matched_data, exposure_col = spatial_matching(fwi_data, exposure_data, target_date=None)
        
        # 3. Calculate vulnerability
        # Choose method: 'simple', 'normalized', or 'weighted'
        vulnerability_data = calculate_vulnerability(matched_data, vulnerability_method='normalized')
        
        # 4. Create visualization maps
        vulnerability_data = create_vulnerability_maps(vulnerability_data, exposure_col, save_data=True)
        
        # 5. Generate summary analysis
        create_vulnerability_summary(vulnerability_data)
        
        print("\nVulnerability analysis completed successfully!")
        print("Generated files:")
        print("   • fwi_vulnerability_analysis_mainland.png - Comprehensive vulnerability maps")
        print("   • vulnerability_analysis_results_mainland.csv - Matched data with risk calculations")
        
        # 6. Print final summary statistics
        print(f"\nFINAL ANALYSIS SUMMARY:")
        print(f"   • Total mainland points processed: {len(vulnerability_data):,}")
        print(f"   • Spatial coverage: {vulnerability_data['latitude'].min():.2f}° to {vulnerability_data['latitude'].max():.2f}°N, ")
        print(f"     {abs(vulnerability_data['longitude'].max()):.2f}° to {abs(vulnerability_data['longitude'].min()):.2f}°W")
        print(f"   • FWI range: {vulnerability_data['fwi_value'].min():.1f} to {vulnerability_data['fwi_value'].max():.1f}")
        print(f"   • Exposure range: {vulnerability_data['exposure_value'].min():.2e} to {vulnerability_data['exposure_value'].max():.2e}")
        
        # Risk category breakdown
        risk_summary = vulnerability_data['risk_category'].value_counts()
        print(f"   • Risk distribution:")
        for category in ['Very High', 'High', 'Moderate', 'Low', 'Very Low']:
            if category in risk_summary:
                count = risk_summary[category]
                percentage = count / len(vulnerability_data) * 100
                print(f"     - {category}: {count:,} points ({percentage:.1f}%)")
        
    except Exception as e:
        print(f"Error in vulnerability analysis: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()